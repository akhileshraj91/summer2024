{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.patches import Rectangle, Polygon\n",
    "from IPython.display import clear_output\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import datetime\n",
    "import csv\n",
    "import os \n",
    "import sys\n",
    "import pandas as pd\n",
    "import gym\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File to write down actor and critic losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")  # Format the current time\n",
    "output_dir = f'./output'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "loss_file = open(f'{output_dir}/losses_seed_{current_time}.csv', mode='w', newline='')\n",
    "loss_writer = csv.writer(loss_file)\n",
    "loss_writer.writerow(['iteration', 'actor_loss', 'critic_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_power_with_wraparound(current, previous, time_diff, wraparound_value=262143.328850):\n",
    "    diff = current - previous\n",
    "    if diff < 0:  # Wraparound detected\n",
    "        diff = (wraparound_value - previous) + current\n",
    "    return diff / time_diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_power(pubEnergy):\n",
    "    power = {}\n",
    "    geopm_sensor0 = geopm_sensor1 = pd.DataFrame({'timestamp':[],'value':[]})\n",
    "    for i,row in pubEnergy.iterrows():\n",
    "        if i%2 == 0:\n",
    "            geopm_sensor0 = pd.concat([geopm_sensor0, pd.DataFrame([{'timestamp': row['time'], 'value': row['value']}])], ignore_index=True)\n",
    "        else:\n",
    "            geopm_sensor1 = pd.concat([geopm_sensor1, pd.DataFrame([{'timestamp': row['time'], 'value': row['value']}])], ignore_index=True)\n",
    "\n",
    "\n",
    "    power['geopm_power_0'] = pd.DataFrame({\n",
    "        'timestamp': geopm_sensor0['timestamp'][1:],  # Add timestamps\n",
    "        'power': [\n",
    "            calculate_power_with_wraparound(\n",
    "                geopm_sensor0['value'][i],\n",
    "                geopm_sensor0['value'][i-1],\n",
    "                geopm_sensor0['timestamp'][i] - geopm_sensor0['timestamp'][i-1]\n",
    "            ) for i in range(1, len(geopm_sensor0))\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    # Apply the same logic to geopm_power_1\n",
    "    power['geopm_power_1'] = pd.DataFrame({\n",
    "        'timestamp': geopm_sensor1['timestamp'][1:],  # Add timestamps\n",
    "        'power': [\n",
    "            calculate_power_with_wraparound(\n",
    "                geopm_sensor1['value'][i],\n",
    "                geopm_sensor1['value'][i-1],\n",
    "                geopm_sensor1['timestamp'][i] - geopm_sensor1['timestamp'][i-1]\n",
    "            ) for i in range(1, len(geopm_sensor1))\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    min_length = min(len(power['geopm_power_0']), len(power['geopm_power_1']))\n",
    "    geopm_power_0 = power['geopm_power_0'][:min_length]\n",
    "    geopm_power_1 = power['geopm_power_1'][:min_length]\n",
    "\n",
    "    average_power = pd.DataFrame({\n",
    "        'timestamp': geopm_power_0['timestamp'],  # Use the timestamp from geopm_power_0\n",
    "        'average_power': [(p0 + p1) / 2 for p0, p1 in zip(geopm_power_0['power'], geopm_power_1['power'])]\n",
    "    })\n",
    "    average_power['elapsed_time'] = average_power['timestamp'] - average_power['timestamp'].iloc[0]\n",
    "    power['average_power'] = average_power\n",
    "    return power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_progress(progress_data, energy_data):\n",
    "    Progress_DATA = {} \n",
    "    progress_sensor = pd.DataFrame(progress_data)\n",
    "    first_sensor_point = min(energy_data['average_power']['timestamp'].iloc[0], progress_sensor['time'][0])\n",
    "    progress_sensor['elapsed_time'] = progress_sensor['time'] - first_sensor_point  # New column for elapsed time\n",
    "    # progress_sensor = progress_sensor.set_index('elapsed_time')\n",
    "    performance_elapsed_time = progress_sensor.elapsed_time\n",
    "    # Add performance_frequency as a new column in progress_sensor\n",
    "    frequency_values = [\n",
    "        progress_data['value'].iloc[t] / (performance_elapsed_time[t] - performance_elapsed_time[t-1]) for t in range(1, len(performance_elapsed_time))\n",
    "    ]\n",
    "    \n",
    "    # Ensure the frequency_values length matches the index length\n",
    "    frequency_values = [0] + frequency_values  # Prepend a 0 for the first index\n",
    "    progress_sensor['frequency'] = frequency_values\n",
    "    upsampled_timestamps= energy_data['average_power']['timestamp']\n",
    "    \n",
    "    # true_count = (progress_sensor['time'] <= upsampled_timestamps.iloc[0]).sum()\n",
    "\n",
    "    progress_frequency_median = pd.DataFrame({'median': np.nanmedian(progress_sensor['frequency'].where(progress_sensor['time'] <= upsampled_timestamps.iloc[0])), 'timestamp': upsampled_timestamps.iloc[0]}, index=[0])\n",
    "    for t in range(1, len(upsampled_timestamps)):\n",
    "        progress_frequency_median = pd.concat([progress_frequency_median, pd.DataFrame({'median': [np.nanmedian(progress_sensor['frequency'].where((progress_sensor['time'] >= upsampled_timestamps.iloc[t-1]) & (progress_sensor['time'] <= upsampled_timestamps.iloc[t])))],\n",
    "        'timestamp': [upsampled_timestamps.iloc[t]]})], ignore_index=True)\n",
    "    progress_frequency_median['elapsed_time'] = progress_frequency_median['timestamp'] - progress_frequency_median['timestamp'].iloc[0]\n",
    "    # Assign progress_frequency_median as a new column\n",
    "    Progress_DATA['progress_sensor'] = progress_sensor\n",
    "    Progress_DATA['progress_frequency_median'] = progress_frequency_median\n",
    "    return Progress_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_papi(PAPI_data):\n",
    "    PAPI = {}\n",
    "    for scope in PAPI_data['scope'].unique():\n",
    "        # Extract the string between the 3rd and 4th dots\n",
    "        scope_parts = scope.split('.')\n",
    "        if len(scope_parts) > 4:  # Ensure there are enough parts\n",
    "            extracted_scope = scope_parts[3]\n",
    "            # Aggregate the data for the extracted scope using pd.concat\n",
    "            PAPI[extracted_scope] = PAPI_data[PAPI_data['scope'] == scope]\n",
    "            instantaneous_values = [0] + [PAPI[extracted_scope]['value'].iloc[k] - PAPI[extracted_scope]['value'].iloc[k-1] for k in range(1,len(PAPI[extracted_scope]))]\n",
    "            # Normalize the instantaneous values between 0 and 10\n",
    "            # min_val = min(instantaneous_values)\n",
    "            # max_val = max(instantaneous_values)\n",
    "            PAPI[extracted_scope]['instantaneous_value'] = instantaneous_values\n",
    "            PAPI[extracted_scope]['elapsed_time'] = PAPI[extracted_scope]['time'] - PAPI[extracted_scope]['time'].iloc[0]\n",
    "    return PAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derived_papi(PAPI_data):\n",
    "    DERIVED = {}\n",
    "    DERIVED['TOT_INS_PER_CYC'] = np.array(PAPI_data['PAPI_TOT_INS']['instantaneous_value']) / np.array(PAPI_data['PAPI_TOT_CYC']['instantaneous_value'])\n",
    "    DERIVED['TOT_CYC_PER_INS'] = np.array(PAPI_data['PAPI_TOT_CYC']['instantaneous_value']) / np.array(PAPI_data['PAPI_TOT_INS']['instantaneous_value'])\n",
    "    DERIVED['L3_TCM_PER_TCA'] = np.array(PAPI_data['PAPI_L3_TCM']['instantaneous_value']) / np.array(PAPI_data['PAPI_L3_TCA']['instantaneous_value'])  \n",
    "    DERIVED['TOT_STL_PER_CYC'] = np.array(PAPI_data['PAPI_RES_STL']['instantaneous_value']) / np.array(PAPI_data['PAPI_TOT_CYC']['instantaneous_value']) \n",
    "    DERIVED['TIME_STAMP'] = np.array(PAPI_data['PAPI_TOT_INS']['elapsed_time'])\n",
    "    return DERIVED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(traces):\n",
    "    normalized_PAPI = {}\n",
    "    max_value = {'PAPI_L3_TCA': float('-inf'), 'PAPI_TOT_INS': float('-inf'), 'PAPI_TOT_CYC': float('-inf'), 'PAPI_RES_STL': float('-inf'), 'PAPI_L3_TCM': float('-inf')}\n",
    "    min_value = {'PAPI_L3_TCA': float('inf'), 'PAPI_TOT_INS': float('inf'), 'PAPI_TOT_CYC': float('inf'), 'PAPI_RES_STL': float('inf'), 'PAPI_L3_TCM': float('inf')}\n",
    "    for app in traces.keys():\n",
    "        for trace in traces[app].keys():\n",
    "            for scope in traces[app][trace]['papi'].keys():\n",
    "                max_value[scope] = max(max_value[scope],max(traces[app][trace]['papi'][scope]['instantaneous_value']))\n",
    "                min_value[scope] = min(min_value[scope],min(traces[app][trace]['papi'][scope]['instantaneous_value']))\n",
    "    for app in traces.keys():\n",
    "        for trace in traces[app].keys():\n",
    "            for scope in traces[app][trace]['papi'].keys():\n",
    "                traces[app][trace]['papi'][scope]['normalized_value'] = [(value - min_value[scope]) / (max_value[scope] - min_value[scope]) * 10 for value in traces[app][trace]['papi'][scope]['instantaneous_value']]\n",
    "    return traces\n",
    "\n",
    "def generate_PCAP(PCAP_data):\n",
    "    for row in PCAP_data.iterrows():\n",
    "        if row[1]['time'] == 0:\n",
    "            PCAP_data = PCAP_data.drop(row[0])\n",
    "\n",
    "\n",
    "    PCAP_data['elapsed_time'] = PCAP_data['time'] - PCAP_data['time'].iloc[0]\n",
    "    return PCAP_data\n",
    "\n",
    "def get_data_dir():\n",
    "    # current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    current_dir = os.getcwd()\n",
    "    print(current_dir)\n",
    "    return os.path.join(current_dir, \"experiment_data\", \"data_generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/akhileshraj/Desktop/summer2024/main_codes\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = get_data_dir()\n",
    "root,folders,files = next(os.walk(DATA_DIR))\n",
    "training_data = {}\n",
    "for APP in folders:\n",
    "    APP_DIR = os.path.join(DATA_DIR, APP)\n",
    "    training_data[APP] = {}\n",
    "    for file in next(os.walk(APP_DIR))[2]:\n",
    "        training_data[APP][file] = {}\n",
    "        if file.endswith('.tar'):\n",
    "            tar_path = os.path.join(APP_DIR, file)\n",
    "            extract_dir = os.path.join(APP_DIR, file[:-4])  \n",
    "            \n",
    "            if not os.path.exists(extract_dir):\n",
    "                os.makedirs(extract_dir)\n",
    "            \n",
    "            with tarfile.open(tar_path, 'r') as tar:\n",
    "                tar.extractall(path=extract_dir)\n",
    "            \n",
    "        pubProgress = pd.read_csv(f'{extract_dir}/progress.csv')\n",
    "        pubEnergy = pd.read_csv(f'{extract_dir}/energy.csv')\n",
    "        pubPAPI = pd.read_csv(f'{extract_dir}/papi.csv')\n",
    "        pubPCAP = pd.read_csv(f'{extract_dir}/PCAP_file.csv')\n",
    "        # with open(f'{extract_dir}/parameters.yaml', 'r') as f:\n",
    "        #     yaml = YAML(typ='safe', pure=True)\n",
    "        #     parameters = yaml.load(f)\n",
    "        #     PCAP = parameters['PCAP']\n",
    "        # training_data['data']['PCAP'] = pd.read_csv(f'{extract_dir}/PCAP_file.csv')\n",
    "        training_data[APP][file]['power'] = compute_power(pubEnergy)\n",
    "        training_data[APP][file]['progress'] = measure_progress(pubProgress,training_data[APP][file]['power'])\n",
    "        training_data[APP][file]['papi'] = collect_papi(pubPAPI)\n",
    "        training_data[APP][file]['PCAP'] = generate_PCAP(pubPCAP)\n",
    "        training_data[APP][file]['derived_papi'] = derived_papi(training_data[APP][file]['papi'])   \n",
    "        # print(training_data[APP][file]['PCAP'] )    \n",
    "# training_data = normalize(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TOT_INS_PER_CYC': array([       nan, 0.14205005, 0.1420557 , 0.14185485, 0.14107412,\n",
       "        0.14100823, 0.14106961, 0.14073528, 0.141457  , 0.14166151,\n",
       "        0.14204819, 0.14194053, 0.14161753, 0.14176039, 0.14189252,\n",
       "        0.14175197, 0.14186593, 0.14166507, 0.14160525, 0.14162423,\n",
       "        0.14154042, 0.14170791, 0.14145823, 0.14176053, 0.14184874,\n",
       "        0.14174819, 0.14152713, 0.14174298, 0.14161214, 0.1418489 ,\n",
       "        0.14176668, 0.14175412, 0.14158147, 0.1420234 , 0.14177071,\n",
       "        0.1416144 , 0.14154172, 0.14179025, 0.1415452 , 0.14169732,\n",
       "        0.14171472, 0.14160207, 0.14161833, 0.14179675, 0.14168404,\n",
       "        0.14164377, 0.14173407, 0.14165993, 0.14161189, 0.14171468,\n",
       "        0.14164428, 0.14157307, 0.14167099, 0.14166712, 0.14161321,\n",
       "        0.14156901, 0.14170521, 0.14155466, 0.14170473, 0.14176603,\n",
       "        0.14203591, 0.14155918, 0.14165992, 0.1416785 , 0.14151026,\n",
       "        0.14159084, 0.14162665, 0.14165089, 0.14177625, 0.14155375,\n",
       "        0.14163959, 0.14169557, 0.14157101, 0.14158803, 0.14160179,\n",
       "        0.14195064, 0.14116723, 0.14101062, 0.14104526, 0.14123216,\n",
       "        0.14119146, 0.1409447 , 0.14104118, 0.14102989, 0.14099742,\n",
       "        0.14108377, 0.14086747, 0.14096973, 0.14110805, 0.14118878,\n",
       "        0.14103691, 0.14106647, 0.14088778, 0.14098688, 0.14108165,\n",
       "        0.14089574, 0.1410045 , 0.1410899 , 0.14103833, 0.14092074,\n",
       "        0.14107464, 0.14098082, 0.14088335, 0.14108065, 0.14152864,\n",
       "        0.14122025, 0.14133961, 0.14118648, 0.14140226, 0.14131138,\n",
       "        0.1414198 , 0.14132294, 0.14137996, 0.14139654, 0.14127448,\n",
       "        0.14117795, 0.14140964, 0.14120056, 0.14142908, 0.14104257,\n",
       "        0.1405772 , 0.14054923, 0.14052887, 0.14046574, 0.14042657,\n",
       "        0.14059297, 0.14038779, 0.14058602, 0.14057392, 0.14043214,\n",
       "        0.14054701, 0.14040642, 0.14049181, 0.14045434, 0.1405932 ,\n",
       "        0.14044856, 0.14038975, 0.1402424 , 0.14046772, 0.14025665,\n",
       "        0.14047329, 0.1404247 , 0.14042153, 0.14049775, 0.14046483,\n",
       "        0.14062633, 0.14093508, 0.14120435, 0.14126374, 0.14113375,\n",
       "        0.14131666, 0.1412147 , 0.14132809, 0.14107629, 0.14126181,\n",
       "        0.14116171, 0.14099757, 0.14096641, 0.14107598, 0.14108603,\n",
       "        0.14104605, 0.14093264, 0.14105692, 0.140963  , 0.14112392,\n",
       "        0.14098259, 0.14099071, 0.14087287, 0.14107505, 0.1414806 ,\n",
       "        0.14146438, 0.14151801, 0.14139113, 0.14148892, 0.14156432,\n",
       "        0.14143918, 0.14157388, 0.14146835, 0.14147706, 0.1414886 ,\n",
       "        0.14155853, 0.14149154, 0.14150197, 0.1414729 , 0.14171035,\n",
       "        0.14149171, 0.14150488, 0.14186211, 0.14152158, 0.14143375,\n",
       "        0.14160467, 0.14163034, 0.14158985, 0.14146278, 0.14153209,\n",
       "        0.14149795, 0.14149457, 0.14157749, 0.1414846 , 0.14135651,\n",
       "        0.14156443, 0.14140407, 0.14182712, 0.14153574, 0.14145691,\n",
       "        0.14161591, 0.1414259 , 0.1414552 , 0.14164043, 0.14141987,\n",
       "        0.14164063, 0.14139114, 0.1414715 , 0.14150404, 0.14156645,\n",
       "        0.14155727, 0.14152159, 0.14164912, 0.14149869, 0.14149641,\n",
       "        0.14158014, 0.14136715, 0.14153495, 0.14139533, 0.14151895,\n",
       "        0.14151692, 0.14147294, 0.14150593, 0.14155847, 0.14145232,\n",
       "        0.14144521, 0.14158744, 0.14139985, 0.14148941, 0.14142344,\n",
       "        0.14079868, 0.14096948, 0.14079716, 0.14094031, 0.14071039,\n",
       "        0.14089954, 0.14084506, 0.14074901, 0.14067924, 0.14079212,\n",
       "        0.14145457, 0.14141631, 0.14123162, 0.14126608, 0.14149991,\n",
       "        0.14102574, 0.14105694, 0.1409786 , 0.14108638, 0.14105547,\n",
       "        0.1410694 , 0.14112543, 0.14096591, 0.14114514, 0.14101737,\n",
       "        0.14118278, 0.14100411, 0.14110889, 0.14102531, 0.14134097,\n",
       "        0.14111356, 0.14031159, 0.14031285, 0.1403789 , 0.14044671,\n",
       "        0.14060489, 0.14098793, 0.14098139, 0.14099888, 0.14119315,\n",
       "        0.14102841, 0.14094535, 0.14114453, 0.14108518, 0.14113436,\n",
       "        0.14039925, 0.14039614, 0.14063588, 0.14029509, 0.13992204,\n",
       "        0.14004952, 0.13990761, 0.13998728, 0.13985344, 0.1397797 ,\n",
       "        0.14000534, 0.13987791,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan]),\n",
       " 'TOT_CYC_PER_INS': array([       nan, 7.03977227, 7.03949222, 7.04945946, 7.08847236,\n",
       "        7.09178457, 7.0886988 , 7.10553896, 7.06928626, 7.05908039,\n",
       "        7.03986449, 7.04520404, 7.06127272, 7.05415642, 7.04758773,\n",
       "        7.05457563, 7.04890859, 7.05890315, 7.06188516, 7.06093874,\n",
       "        7.06511939, 7.05676928, 7.06922445, 7.05414995, 7.049763  ,\n",
       "        7.05476396, 7.06578284, 7.05502301, 7.06154135, 7.04975521,\n",
       "        7.05384388, 7.05446884, 7.0630714 , 7.04109328, 7.05364313,\n",
       "        7.06142895, 7.06505486, 7.05267106, 7.06488129, 7.05729647,\n",
       "        7.05642998, 7.06204388, 7.06123273, 7.05234777, 7.05795784,\n",
       "        7.05996467, 7.05546681, 7.05915942, 7.061554  , 7.05643183,\n",
       "        7.05993939, 7.06349026, 7.05860811, 7.05880102, 7.06148832,\n",
       "        7.06369259, 7.05690354, 7.06440903, 7.05692732, 7.05387613,\n",
       "        7.0404731 , 7.0641834 , 7.05915967, 7.05823391, 7.06662539,\n",
       "        7.06260383, 7.06081818, 7.05960968, 7.05336775, 7.06445435,\n",
       "        7.06017274, 7.05738383, 7.06359305, 7.06274398, 7.06205769,\n",
       "        7.04470244, 7.08379678, 7.09166451, 7.08992269, 7.08054009,\n",
       "        7.08258153, 7.09498119, 7.0901278 , 7.0906956 , 7.09232846,\n",
       "        7.0879875 , 7.09887103, 7.09372148, 7.08676785, 7.08271562,\n",
       "        7.09034229, 7.08885655, 7.09784783, 7.09285838, 7.08809422,\n",
       "        7.09744666, 7.09197209, 7.08767975, 7.09027113, 7.09618768,\n",
       "        7.08844612, 7.09316345, 7.09807061, 7.08814432, 7.06570744,\n",
       "        7.08113738, 7.0751577 , 7.08283135, 7.0720228 , 7.07657069,\n",
       "        7.07114571, 7.07599205, 7.07313805, 7.07230876, 7.07841935,\n",
       "        7.08325917, 7.07165375, 7.08212467, 7.07068168, 7.09005803,\n",
       "        7.11352893, 7.11494485, 7.11597539, 7.11917351, 7.12115935,\n",
       "        7.11273126, 7.12312659, 7.11308258, 7.11369508, 7.1208768 ,\n",
       "        7.11505686, 7.12218143, 7.1178528 , 7.1197513 , 7.11271938,\n",
       "        7.1200446 , 7.12302724, 7.13051103, 7.11907355, 7.12978656,\n",
       "        7.11879116, 7.12125413, 7.12141527, 7.11755193, 7.11922004,\n",
       "        7.1110439 , 7.09546569, 7.08193464, 7.07895734, 7.08547746,\n",
       "        7.07630675, 7.08141563, 7.07573434, 7.08836352, 7.07905402,\n",
       "        7.08407413, 7.09232068, 7.09388874, 7.08837866, 7.08787403,\n",
       "        7.08988309, 7.09558859, 7.08933689, 7.09406032, 7.08597078,\n",
       "        7.09307457, 7.09266577, 7.09859869, 7.08842573, 7.06810687,\n",
       "        7.06891745, 7.06623851, 7.07257939, 7.067691  , 7.06392707,\n",
       "        7.07017686, 7.0634496 , 7.06871874, 7.06828388, 7.06770705,\n",
       "        7.06421596, 7.06756028, 7.06703937, 7.06849174, 7.05664756,\n",
       "        7.0675519 , 7.06689401, 7.04909854, 7.06606023, 7.07044805,\n",
       "        7.06191396, 7.0606338 , 7.06265336, 7.06899718, 7.06553527,\n",
       "        7.0672404 , 7.06740877, 7.06326996, 7.06790716, 7.07431184,\n",
       "        7.06392127, 7.07193204, 7.05083763, 7.06535329, 7.06929047,\n",
       "        7.06135358, 7.07084066, 7.06937579, 7.06013089, 7.07114228,\n",
       "        7.060121  , 7.07257886, 7.06856161, 7.06693603, 7.06382076,\n",
       "        7.06427857, 7.06605948, 7.05969811, 7.06720304, 7.067317  ,\n",
       "        7.06313741, 7.0737794 , 7.06539264, 7.07236931, 7.0661914 ,\n",
       "        7.06629277, 7.0684897 , 7.06684177, 7.06421896, 7.06951998,\n",
       "        7.0698754 , 7.06277346, 7.07214326, 7.06766695, 7.07096344,\n",
       "        7.10233955, 7.09373392, 7.10241604, 7.09520204, 7.10679565,\n",
       "        7.09725522, 7.10000027, 7.10484559, 7.10836915, 7.10267001,\n",
       "        7.0694075 , 7.07132028, 7.08056752, 7.07884011, 7.06714229,\n",
       "        7.090904  , 7.08933565, 7.09327532, 7.08785629, 7.08940972,\n",
       "        7.08870973, 7.08589508, 7.09391384, 7.08490578, 7.09132474,\n",
       "        7.08301691, 7.09199201, 7.08672586, 7.09092587, 7.07508947,\n",
       "        7.0864911 , 7.126995  , 7.12693109, 7.12357772, 7.12013836,\n",
       "        7.1121283 , 7.09280595, 7.09313492, 7.09225516, 7.08249652,\n",
       "        7.09076975, 7.09494836, 7.0849365 , 7.08791648, 7.08544701,\n",
       "        7.12254541, 7.12270291, 7.11056096, 7.12783338, 7.14683685,\n",
       "        7.14033159, 7.14757383, 7.14350614, 7.1503426 , 7.15411483,\n",
       "        7.14258466, 7.14909164,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan]),\n",
       " 'L3_TCM_PER_TCA': array([       nan, 0.93337794, 0.93035372, 0.93528239, 0.93228977,\n",
       "        0.9322783 , 0.93231608, 0.93532666, 0.93060337, 0.93596144,\n",
       "        0.92962704, 0.93411888, 0.9351383 , 0.92944283, 0.93433798,\n",
       "        0.93286177, 0.93561771, 0.93110562, 0.93287499, 0.93266971,\n",
       "        0.93328694, 0.93060044, 0.93627148, 0.92979958, 0.93663063,\n",
       "        0.92930519, 0.93618407, 0.93030872, 0.93279259, 0.9325959 ,\n",
       "        0.93547018, 0.93086364, 0.93559181, 0.93220771, 0.93112357,\n",
       "        0.93313116, 0.93452485, 0.93060119, 0.93500142, 0.9323322 ,\n",
       "        0.93193329, 0.93334139, 0.93255798, 0.93526676, 0.93156676,\n",
       "        0.93157833, 0.935113  , 0.9331895 , 0.93291475, 0.93111604,\n",
       "        0.93183392, 0.9334128 , 0.93353915, 0.93242343, 0.93254125,\n",
       "        0.93549575, 0.93033684, 0.93507465, 0.93296446, 0.92920549,\n",
       "        0.93411539, 0.93431804, 0.93260677, 0.93128423, 0.93441449,\n",
       "        0.93355514, 0.93247415, 0.93099171, 0.93194379, 0.93552548,\n",
       "        0.93176371, 0.93085655, 0.93237745, 0.93303255, 0.93593192,\n",
       "        0.93332933, 0.92979871, 0.93482287, 0.93360971, 0.93468297,\n",
       "        0.93075049, 0.9357091 , 0.93343886, 0.93319216, 0.93309258,\n",
       "        0.93202336, 0.93566431, 0.93170993, 0.93089943, 0.9358271 ,\n",
       "        0.93217535, 0.93172696, 0.93586186, 0.93269469, 0.93081691,\n",
       "        0.93597307, 0.93316818, 0.93191176, 0.93173617, 0.93424397,\n",
       "        0.9324053 , 0.93241858, 0.93670318, 0.92993961, 0.93321268,\n",
       "        0.93462553, 0.93251231, 0.93575986, 0.93014573, 0.93332487,\n",
       "        0.93568856, 0.93320216, 0.93236201, 0.93168224, 0.93332419,\n",
       "        0.9356433 , 0.93178889, 0.93512785, 0.93161312, 0.93385498,\n",
       "        0.93376323, 0.9343712 , 0.93465815, 0.93422038, 0.93588372,\n",
       "        0.93140738, 0.93628781, 0.93252407, 0.93400765, 0.93512418,\n",
       "        0.93281151, 0.93712756, 0.93259423, 0.93541895, 0.93332101,\n",
       "        0.93193625, 0.9344993 , 0.93655294, 0.93211507, 0.93688963,\n",
       "        0.93426778, 0.93435118, 0.93352877, 0.9333953 , 0.93589588,\n",
       "        0.93263149, 0.93373668, 0.93380075, 0.93390993, 0.93658929,\n",
       "        0.93329904, 0.93573245, 0.93209397, 0.93326406, 0.93686669,\n",
       "        0.93080783, 0.9333436 , 0.93596079, 0.93262038, 0.9332663 ,\n",
       "        0.93355032, 0.93564538, 0.93309777, 0.93488934, 0.93242651,\n",
       "        0.93390668, 0.93312344, 0.93665303, 0.93027718, 0.93385592,\n",
       "        0.93446961, 0.93253558, 0.9354124 , 0.9348709 , 0.93157852,\n",
       "        0.9340809 , 0.93268847, 0.93309741, 0.93443432, 0.93262032,\n",
       "        0.93401435, 0.93466337, 0.93286644, 0.93376495, 0.93478735,\n",
       "        0.9328989 , 0.93400315, 0.93369425, 0.93492185, 0.93631535,\n",
       "        0.93340137, 0.93237809, 0.93401963, 0.93490332, 0.93266741,\n",
       "        0.93537179, 0.93243878, 0.93159124, 0.93356038, 0.93660148,\n",
       "        0.93144993, 0.93573702, 0.9306803 , 0.93424428, 0.93535622,\n",
       "        0.93135738, 0.93630279, 0.93292244, 0.93174909, 0.93572324,\n",
       "        0.93249391, 0.9367984 , 0.93398214, 0.93367402, 0.93263642,\n",
       "        0.93379243, 0.9345623 , 0.93022398, 0.93761524, 0.93308051,\n",
       "        0.93110295, 0.93683483, 0.93170166, 0.93503724, 0.93219061,\n",
       "        0.93270311, 0.93303465, 0.93283468, 0.93339913, 0.93437457,\n",
       "        0.93632802, 0.93130636, 0.93576938, 0.9326181 , 0.93415776,\n",
       "        0.93529421, 0.93203885, 0.93688974, 0.9306322 , 0.93698243,\n",
       "        0.93303497, 0.93377005, 0.93363693, 0.9375936 , 0.93301859,\n",
       "        0.93482422, 0.93105626, 0.93363961, 0.93641016, 0.93123362,\n",
       "        0.93521147, 0.93382823, 0.93446354, 0.93350319, 0.93525866,\n",
       "        0.93347622, 0.93236377, 0.93657922, 0.93189628, 0.93444314,\n",
       "        0.93366277, 0.93631266, 0.93097006, 0.93498277, 0.93589683,\n",
       "        0.93155365, 0.936131  , 0.93541114, 0.93448105, 0.93335389,\n",
       "        0.93735375, 0.93362805, 0.93447237, 0.93357537, 0.93143264,\n",
       "        0.93450418, 0.936839  , 0.93240337, 0.93384392, 0.9356682 ,\n",
       "        0.93458706, 0.93634092, 0.93694585, 0.93263637, 0.93711639,\n",
       "        0.93451474, 0.93654664, 0.93370273, 0.9354386 , 0.93830482,\n",
       "        0.93268321, 0.93406945,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan]),\n",
       " 'TOT_STL_PER_CYC': array([       nan, 0.89636459, 0.88405695, 0.89913449, 0.88948566,\n",
       "        0.88969631, 0.89813798, 0.89001666, 0.89235019, 0.89139356,\n",
       "        0.89246424, 0.88492427, 0.90195254, 0.89668454, 0.89413455,\n",
       "        0.89882153, 0.89563677, 0.90408914, 0.89845688, 0.90277114,\n",
       "        0.89525885, 0.89977585, 0.88830992, 0.90482339, 0.89239644,\n",
       "        0.89271784, 0.90462298, 0.89624114, 0.89940003, 0.88767595,\n",
       "        0.89948828, 0.89571727, 0.89358949, 0.8993015 , 0.89574941,\n",
       "        0.89845207, 0.89431338, 0.90525917, 0.89673458, 0.89453844,\n",
       "        0.90437324, 0.89423362, 0.89874629, 0.89010178, 0.89620621,\n",
       "        0.89494147, 0.89130792, 0.90331489, 0.894752  , 0.89948169,\n",
       "        0.90435958, 0.89338215, 0.89001601, 0.89916071, 0.89428725,\n",
       "        0.90219752, 0.89188621, 0.90554541, 0.89424591, 0.89305344,\n",
       "        0.89706834, 0.89422044, 0.89889278, 0.89538922, 0.89776764,\n",
       "        0.89368472, 0.89552668, 0.90007257, 0.89510467, 0.90134864,\n",
       "        0.89544519, 0.89909521, 0.88702347, 0.89842196, 0.90103889,\n",
       "        0.89170028, 0.90193082, 0.89280155, 0.90304941, 0.88774052,\n",
       "        0.89623454, 0.89235478, 0.89471543, 0.89027097, 0.88995437,\n",
       "        0.89586592, 0.90129812, 0.89132177, 0.89669027, 0.88412912,\n",
       "        0.89950056, 0.8913567 , 0.88915626, 0.89469531, 0.89231503,\n",
       "        0.8967023 , 0.89014454, 0.89988839, 0.88655934, 0.88936256,\n",
       "        0.89955034, 0.89058588, 0.8923192 , 0.89235479, 0.89066733,\n",
       "        0.89189205, 0.89324644, 0.90026158, 0.89068572, 0.89240602,\n",
       "        0.87636509, 0.89230747, 0.88411623, 0.89396898, 0.8879716 ,\n",
       "        0.89056131, 0.89815782, 0.89135847, 0.88866563, 0.88452788,\n",
       "        0.89354531, 0.8886951 , 0.89265981, 0.88492461, 0.88791289,\n",
       "        0.89103547, 0.88796926, 0.8941022 , 0.88968678, 0.89686964,\n",
       "        0.88630219, 0.89590827, 0.88148232, 0.88824054, 0.87964733,\n",
       "        0.88673976, 0.88981073, 0.88411195, 0.89077528, 0.88383153,\n",
       "        0.88494656, 0.89355516, 0.87698044, 0.89371767, 0.88899924,\n",
       "        0.89871121, 0.88199939, 0.88450739, 0.89323391, 0.88734189,\n",
       "        0.89280684, 0.88724732, 0.88989208, 0.86794941, 0.86995315,\n",
       "        0.8822588 , 0.87705391, 0.888015  , 0.87179316, 0.88540488,\n",
       "        0.87635873, 0.87502549, 0.88054334, 0.87579736, 0.88142743,\n",
       "        0.87692097, 0.87604068, 0.87512722, 0.87370963, 0.88123037,\n",
       "        0.88569013, 0.89178794, 0.88473401, 0.89545282, 0.88700358,\n",
       "        0.89518246, 0.88707848, 0.8867267 , 0.89445242, 0.88797912,\n",
       "        0.89462118, 0.8858075 , 0.88359042, 0.89497037, 0.88328333,\n",
       "        0.8968465 , 0.89010384, 0.89915961, 0.89943884, 0.90205612,\n",
       "        0.9088077 , 0.89642362, 0.89540596, 0.89911721, 0.89545289,\n",
       "        0.90273811, 0.89181862, 0.89688394, 0.89549514, 0.89717119,\n",
       "        0.87502392, 0.90234301, 0.88163249, 0.89468043, 0.902739  ,\n",
       "        0.89693709, 0.90237354, 0.89082643, 0.90106184, 0.89767514,\n",
       "        0.89728519, 0.90231378, 0.88683106, 0.89875359, 0.89604504,\n",
       "        0.90400521, 0.89496979, 0.77302471, 0.89755957, 0.88704195,\n",
       "        0.89311053, 0.90218334, 0.88760764, 0.89887539, 0.89203929,\n",
       "        0.89982394, 0.89149018, 0.89554769, 0.90447083, 0.89055199,\n",
       "        0.90215416, 0.89702542, 0.89480668, 0.88640968, 0.88994952,\n",
       "        0.88173459, 0.88861122, 0.90318398, 0.88877722, 0.87308585,\n",
       "        0.87981325, 0.89109444, 0.8845238 , 0.89325328, 0.88800911,\n",
       "        0.89581003, 0.87539875, 0.84691579, 0.89887559, 0.87865278,\n",
       "        0.89198706, 0.88507005, 0.85934858, 0.89404267, 0.88870684,\n",
       "        0.88966833, 0.88229256, 0.89670682, 0.88582062, 0.88494544,\n",
       "        0.89847581, 0.88911771, 0.88180974, 0.88523242, 0.88594523,\n",
       "        0.87854872, 0.87938688, 0.87237586, 0.87747437, 0.88276637,\n",
       "        0.87724835, 0.88961027, 0.87756555, 0.88063131, 0.89276235,\n",
       "        0.88562756, 0.88799741, 0.88292673, 0.88570184, 0.87274854,\n",
       "        0.873271  , 0.88794037, 0.88319949, 0.88901815, 0.89683273,\n",
       "        0.89826245, 0.8933185 , 0.89903932, 0.88558916, 0.88704325,\n",
       "        0.89014924, 0.8870858 ,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan]),\n",
       " 'TIME_STAMP': array([  0.        ,   1.00109243,   1.99928856,   3.00077701,\n",
       "          4.00066686,   5.00012064,   6.00009775,   7.00192714,\n",
       "          7.99927258,   9.00112844,   9.99901414,  10.9998064 ,\n",
       "         12.00149775,  12.99805474,  14.00003862,  14.99980903,\n",
       "         16.00148749,  16.99980855,  18.00019145,  19.000422  ,\n",
       "         20.00003076,  20.99856281,  22.00112534,  22.99854064,\n",
       "         24.0013988 ,  24.99795961,  26.00169802,  26.99923992,\n",
       "         27.99883914,  28.99913454,  30.00159383,  30.99934912,\n",
       "         32.00105715,  33.00091791,  33.99908161,  34.99950314,\n",
       "         36.0002799 ,  36.99751735,  38.00001121,  38.99985623,\n",
       "         39.99868321,  40.99902654,  41.99913812,  43.00123811,\n",
       "         43.99918485,  44.99889469,  46.00062966,  47.00062823,\n",
       "         48.00064254,  48.99930406,  49.99815607,  50.99958467,\n",
       "         51.99953961,  52.9991219 ,  53.99949622,  55.0012486 ,\n",
       "         55.99898648,  57.00180817,  58.00156093,  58.99786639,\n",
       "         59.99962854,  61.00052762,  62.0003891 ,  62.99904013,\n",
       "         64.00118089,  65.00187492,  66.00138855,  66.99958396,\n",
       "         67.99878812,  69.00135231,  70.00016284,  70.99948335,\n",
       "         71.99849534,  72.99890232,  74.00170946,  75.00155067,\n",
       "         75.99770927,  76.99954414,  77.99961305,  79.00064349,\n",
       "         79.99819684,  81.00086617,  82.00057483,  83.00016308,\n",
       "         84.00052595,  84.99898314,  86.00184488,  87.00053167,\n",
       "         87.998101  ,  89.00101542,  90.00008965,  90.99885273,\n",
       "         92.00084233,  93.00051904,  93.9979682 ,  95.00113153,\n",
       "         96.00134778,  96.999578  ,  97.99871492,  98.99974298,\n",
       "         99.99911976, 100.99873233, 102.0014298 , 102.99875164,\n",
       "        103.99883509, 104.99995351, 105.99942613, 107.00125003,\n",
       "        107.99826312, 108.99839592, 110.00040436, 111.00036311,\n",
       "        111.999475  , 112.99761939, 113.99793649, 115.00061393,\n",
       "        115.99906349, 117.00096893, 117.99936962, 118.9985981 ,\n",
       "        119.9984467 , 120.99859977, 121.99929738, 122.99914122,\n",
       "        124.00146675, 124.99871683, 126.00072217, 126.99943948,\n",
       "        127.99892592, 129.00023103, 129.99847555, 131.00129819,\n",
       "        131.99985361, 133.00137997, 134.00016713, 134.99835682,\n",
       "        135.99830723, 137.00027514, 137.99848986, 139.00080943,\n",
       "        140.0010519 , 141.00133967, 141.9999795 , 142.99979186,\n",
       "        144.00076795, 144.99941468, 145.99861956, 146.99854112,\n",
       "        147.9983592 , 149.00041461, 149.9998641 , 151.00174212,\n",
       "        151.99986935, 152.99922585, 154.00190163, 154.99919295,\n",
       "        155.99848032, 157.00012374, 157.9998126 , 158.99901295,\n",
       "        159.99875307, 161.00012684, 161.99993944, 163.00066209,\n",
       "        163.99927568, 164.99902987, 165.99927354, 167.00101399,\n",
       "        167.99856448, 168.99856615, 169.99926496, 170.99806261,\n",
       "        172.00042224, 173.00080585, 173.99965191, 174.99986386,\n",
       "        175.99917579, 176.99902034, 177.99973202, 178.99800253,\n",
       "        179.99868822, 180.99923611, 181.99745178, 182.99776959,\n",
       "        183.9992671 , 184.99743128, 185.99819541, 186.99861073,\n",
       "        187.99919057, 189.00201273, 190.00148439, 190.99971533,\n",
       "        191.99979115, 193.00089598, 194.0001471 , 195.00177193,\n",
       "        196.0005312 , 196.99836683, 197.99801826, 199.00138378,\n",
       "        199.99900055, 201.0013895 , 201.99841785, 202.99882174,\n",
       "        204.00058675, 204.99823093, 206.00035644, 207.00001192,\n",
       "        207.99817824, 209.00049233, 209.99852133, 211.00115275,\n",
       "        212.00110722, 213.00197816, 214.000808  , 215.00066853,\n",
       "        216.00116777, 216.99737787, 218.00108647, 219.00120831,\n",
       "        219.99878645, 221.00151086, 221.99970341, 223.00122476,\n",
       "        223.99973893, 224.99956727, 225.99906135, 226.99888086,\n",
       "        227.99854255, 228.9990871 , 230.00183368, 230.9996078 ,\n",
       "        232.00106859, 233.0006628 , 234.00020981, 235.00160789,\n",
       "        235.99925542, 237.00110865, 237.99852753, 239.00041294,\n",
       "        239.99911261, 240.99939895, 241.99761629, 243.00121045,\n",
       "        244.00010133, 245.00113392, 245.99896431, 246.99835515,\n",
       "        248.00057554, 248.99773788, 249.99957848, 250.99932361,\n",
       "        251.99930763, 252.99896979, 254.00052094, 254.99980474,\n",
       "        255.99842024, 257.00044465, 257.99885654, 258.99968314,\n",
       "        259.99922109, 261.00059199, 261.99885654, 262.99947405,\n",
       "        264.00039315, 264.99790859, 266.00012493, 267.00095344,\n",
       "        268.00053644, 268.9989295 , 270.00141954, 271.00164294,\n",
       "        272.00075245, 273.00130653, 273.99757695, 274.99803138,\n",
       "        276.00113201, 276.99915552, 277.99882603, 278.99972701,\n",
       "        279.99912596, 281.00047255, 282.00124955, 282.99877453,\n",
       "        284.00017524, 284.99963856, 286.00004864, 286.99874473,\n",
       "        287.99866366, 289.0014739 , 289.99925876, 290.99673724,\n",
       "        291.99692011, 292.99720311, 293.99647617, 294.99673295,\n",
       "        295.99697661, 296.99721742, 297.99644589, 298.99667668,\n",
       "        299.99690676, 300.99712968, 301.99735188, 302.99656987,\n",
       "        303.99679995, 304.99702954, 305.99724174, 306.99645591,\n",
       "        307.99668145, 308.99692655, 309.99715734, 310.99737334,\n",
       "        311.99662113, 312.99684453, 313.99707389, 314.99730563,\n",
       "        315.99652481, 316.99674749, 317.99696875, 318.99720097,\n",
       "        319.99644041, 320.99666214, 321.99688196, 322.99710155,\n",
       "        323.99733973, 324.99657297, 325.99679947, 326.9970212 ,\n",
       "        327.99723983, 328.99647307, 329.99670482, 330.99692893,\n",
       "        331.99714541, 332.9973681 , 333.99660254, 334.99683404,\n",
       "        335.99706101, 336.99728107, 337.99650025, 338.99673462,\n",
       "        339.99696875, 340.99719334, 341.99641061, 342.99665618,\n",
       "        343.99688315, 344.99711108, 345.99732614, 346.99654746,\n",
       "        347.99677086, 348.99700809, 349.99723506, 350.99645472,\n",
       "        351.99667406, 352.99689102, 353.99711871, 354.9973495 ,\n",
       "        355.99658394, 356.99682355, 357.99705625, 358.9972868 ,\n",
       "        359.99652028, 360.99674344, 361.99698043, 362.9972012 ,\n",
       "        363.99643016, 364.9966588 , 365.99687934, 366.99711514,\n",
       "        367.99733686, 368.99656916, 369.99679613, 370.99701834,\n",
       "        371.99723315, 372.99644899, 373.99667788, 374.99693084,\n",
       "        375.99716115, 376.9973824 , 377.99662209, 378.99685955,\n",
       "        379.99709654, 380.99733067, 381.99656582, 382.99679923,\n",
       "        383.99704146, 384.9972775 , 385.99650502, 386.99673533,\n",
       "        387.9969666 , 388.9972024 , 389.99643993, 390.99667645,\n",
       "        391.99691772, 392.99715328, 393.99738789, 394.99661922,\n",
       "        395.99685049, 396.99708509, 397.9973135 , 398.99654889,\n",
       "        399.99678206, 400.99701262, 401.99723816, 402.99647593,\n",
       "        403.99674392, 404.99700642, 405.99727058, 406.99655509,\n",
       "        407.9968226 , 408.99708796, 409.99735522, 410.99661875,\n",
       "        411.99688387, 412.99715209, 413.99641824, 414.9966867 ,\n",
       "        415.99695206, 416.99721861, 417.99648404, 418.99675727,\n",
       "        419.99702382, 420.9972887 , 421.99655056, 422.99681449,\n",
       "        423.99708414, 424.99734855, 425.99661183, 426.99687195,\n",
       "        427.99713469, 428.9973948 , 429.99665666, 430.99691629,\n",
       "        431.99718332, 432.99644327, 433.99670434, 434.99696589,\n",
       "        435.99722576, 436.99648333, 437.99674344, 438.99703097,\n",
       "        439.99729538, 440.99655485, 441.99681902, 442.99708104,\n",
       "        443.99734473, 444.99660683, 445.99687314, 446.99713802,\n",
       "        447.99740076, 448.99666643, 449.99693203, 450.99719453,\n",
       "        451.99645543, 452.99671745, 453.99698329, 454.99724746,\n",
       "        455.99650693, 456.99676657, 457.99702883, 458.99729013,\n",
       "        459.99655199, 460.99681258, 461.99707508, 462.9973371 ,\n",
       "        463.99660349, 464.99686551, 465.99712801, 466.99738669,\n",
       "        467.99664807, 468.99690866, 469.99716854, 470.99645329,\n",
       "        471.99671793, 472.99697995, 473.99724412, 474.99650884,\n",
       "        475.99677396, 476.9970355 , 477.99729824, 478.99655914,\n",
       "        479.99682188, 480.99708533, 481.99734688, 482.99660707,\n",
       "        483.99687076, 484.99713182, 485.9973948 , 486.99665308,\n",
       "        487.99691272, 488.99717188, 489.99643016, 490.99669123,\n",
       "        491.99695063, 492.99720812, 493.99646544, 494.9967227 ,\n",
       "        495.99698734, 496.99724603, 497.99650621, 498.99676847,\n",
       "        499.99703169, 500.9972918 , 501.99655056, 502.99683475,\n",
       "        503.99710011, 504.99736714, 505.99662995, 506.99689436,\n",
       "        507.99715662, 508.99641657, 509.99668407, 510.99694705,\n",
       "        511.99721074, 512.99647212, 513.99673939, 514.99700475,\n",
       "        515.99727654, 516.99654412, 517.99681258, 518.99707842,\n",
       "        519.99734426, 520.99660778, 521.99687195, 522.99713254,\n",
       "        523.99739695, 524.99666095, 525.99692893, 526.99718928,\n",
       "        527.99645972, 528.99673343, 529.99699831, 530.99726653,\n",
       "        531.99652791, 532.99679065, 533.99705768, 534.99734759,\n",
       "        535.99661374, 536.99687719, 537.9971478 , 538.99641275,\n",
       "        539.99668074, 540.99694967, 541.99721885, 542.99648213,\n",
       "        543.99675417, 544.99702573, 545.9972918 , 546.99656129,\n",
       "        547.99682951, 548.99710011, 549.99736905, 550.9966321 ,\n",
       "        551.99689603, 552.99714541, 553.99738693, 554.99662733,\n",
       "        555.99685907, 556.99708915, 557.99730873, 558.99654007,\n",
       "        559.99677539, 560.99700975, 561.99723268, 562.99645734,\n",
       "        563.99669242, 564.99692822, 565.99714804, 566.99639153,\n",
       "        567.9966259 , 568.99686503, 569.99710011, 570.99732423,\n",
       "        571.99655938, 572.99678922, 573.99702525, 574.99726653,\n",
       "        575.99649572, 576.9967227 , 577.9969523 , 578.99718428,\n",
       "        579.99642944, 580.99666333, 581.99689007, 582.99711657,\n",
       "        583.99735546, 584.99658775, 585.99681044, 586.99703765,\n",
       "        587.99726009, 588.99648976, 589.99672484, 590.996948  ,\n",
       "        591.99717188, 592.99639583, 593.99662781, 594.99686575,\n",
       "        595.99709082, 596.99731493, 597.99653816, 598.99679494,\n",
       "        599.99702883, 600.99725676, 601.99647999, 602.99670124,\n",
       "        603.99693394, 604.99716735, 605.99639034, 606.9966135 ,\n",
       "        607.99684095, 608.99707294, 609.99730182, 610.99612308,\n",
       "        611.99736404, 612.99657822, 613.99680448, 614.99701667,\n",
       "        615.99723339, 616.99643397, 617.99664164, 618.99685335,\n",
       "        619.99706745, 620.99727869, 621.99648905, 622.99669695,\n",
       "        623.99690914, 624.99710989, 625.99733043, 626.99653411,\n",
       "        627.99675608, 628.99696803, 629.99717927, 630.99640989,\n",
       "        631.99663854, 632.99683976, 633.99705005, 634.9972651 ,\n",
       "        635.99647856, 636.99669504, 637.99690533, 638.99710464,\n",
       "        639.99731874, 640.99652076, 641.99673009, 642.99693441,\n",
       "        643.99715066, 644.99735856, 645.99657297, 646.99676824,\n",
       "        647.99698591, 648.99718642, 649.99638796, 650.99658704,\n",
       "        651.99679732, 652.99700665, 653.99721289, 654.99640775,\n",
       "        655.99661708, 656.99681449, 657.99702048, 658.9972167 ,\n",
       "        659.99642253, 660.99663234, 661.99683905, 662.99706197,\n",
       "        663.99727225, 664.99646974, 665.99668193, 666.99688721,\n",
       "        667.9970994 , 668.99731255, 669.99651933, 670.99671674,\n",
       "        671.99693155, 672.99712896, 673.99733734, 674.99653363,\n",
       "        675.99674726, 676.99694777, 677.99715734, 678.99736142,\n",
       "        679.99656606, 680.99676538, 681.99696875, 682.99716663,\n",
       "        683.99737167, 684.99656487, 685.99677134, 686.99697304,\n",
       "        687.99717784, 688.99637628, 689.9965775 , 690.99678254,\n",
       "        691.9969914 , 692.99719524, 693.99640059, 694.9966104 ,\n",
       "        695.99681973, 696.99702263, 697.99722815, 698.99643254,\n",
       "        699.99663997, 700.99684167, 701.99705052, 702.99725699,\n",
       "        703.99646592, 704.99666452, 705.99687552, 706.99708033,\n",
       "        707.99729228, 708.99649906, 709.99670839, 710.99691415,\n",
       "        711.99712682, 712.99733615, 713.99654675, 714.99674249,\n",
       "        715.99695253, 716.99715281, 717.99735761, 718.99656534,\n",
       "        719.9967792 , 720.99697423, 721.99717832, 722.99637866,\n",
       "        723.99658942, 724.99679565, 725.99700022, 726.99722123,\n",
       "        727.99643493, 728.99663806, 729.99684668, 730.99704409,\n",
       "        731.99725723, 732.99646688, 733.99667144, 734.99686742,\n",
       "        735.99707699, 736.99727726, 737.99648762, 738.99668598,\n",
       "        739.99689674, 740.99709845, 741.9973104 , 742.99650884,\n",
       "        743.99671984, 744.99691987, 745.99712253, 746.99731827,\n",
       "        747.99652338, 748.9967196 , 749.99692655, 750.99712253,\n",
       "        751.99732995, 752.99652815, 753.9967339 , 754.99694586,\n",
       "        755.99715352, 756.99735308, 757.99655366, 758.99675941,\n",
       "        759.9969666 , 760.99716663, 761.99637723, 762.9965775 ,\n",
       "        763.99678254, 764.99698067, 765.99718761, 766.99638176,\n",
       "        767.99658775, 768.9967823 , 769.99698853, 770.99718404,\n",
       "        771.99639821, 772.9966011 , 773.99680853, 774.99700284,\n",
       "        775.99721408, 776.98088336])}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training_data[APP][file]['papi']\n",
    "# np.array(training_data[APP][file]['papi']['PAPI_TOT_CYC']['instantaneous_value']) / np.array(training_data[APP][file]['papi']['PAPI_TOT_INS']['instantaneous_value'])\n",
    "# np.array(training_data[APP][file]['papi']['PAPI_TOT_INS']['instantaneous_value']) / np.array(training_data[APP][file]['papi']['PAPI_TOT_CYC']['instantaneous_value'])\n",
    "# np.array(training_data[APP][file]['papi']['PAPI_L3_TCM']['instantaneous_value']) / np.array(training_data[APP][file]['papi']['PAPI_L3_TCA']['instantaneous_value'])\n",
    "# np.array(training_data[APP][file]['papi']['PAPI_RES_STL']['instantaneous_value']) / np.array(training_data[APP][file]['papi']['PAPI_TOT_CYC']['instantaneous_value'])\n",
    "APP = 'ones-stream-add'\n",
    "file = list(training_data[APP].keys())[0]\n",
    "\n",
    "training_data[APP][file]['derived_papi']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_dataset = []\n",
    "old_PCAP = 0\n",
    "action_set = []\n",
    "def get_roi_data(df, time_column, start_time, end_time):\n",
    "    return df[(df[time_column] > start_time) & (df[time_column] <= end_time)]\n",
    "\n",
    "def get_state(training_data, app, trace, start_time, end_time):\n",
    "    ROI_progress = get_roi_data(training_data[app][trace]['progress']['progress_frequency_median'], 'timestamp', start_time, end_time)\n",
    "    ROI_measured_power = get_roi_data(training_data[app][trace]['power']['average_power'], 'timestamp', start_time, end_time)\n",
    "    ROI_derived_1= get_roi_data(training_data[app][trace]['derived_papi']['TOT_INS_PER_CYC'], 'TIME_STAMP', start_time, end_time)\n",
    "    ROI_derived_2 = get_roi_data(training_data[app][trace]['derived_papi']['L3_TCM_PER_TCA'], 'TIME_STAMP', start_time, end_time)\n",
    "    ROI_derived_3 = get_roi_data(training_data[app][trace]['derived_papi']['TOT_STL_PER_CYC'], 'TIME_STAMP', start_time, end_time)\n",
    "\n",
    "    return (\n",
    "        ROI_progress['median'].mean() if not ROI_progress.empty else 0,\n",
    "        ROI_measured_power['average_power'].mean() if not ROI_measured_power.empty else 0,\n",
    "        ROI_derived_1.mean() if not ROI_derived_1.empty else 0,\n",
    "        ROI_derived_2.mean() if not ROI_derived_1.empty else 0,\n",
    "        ROI_derived_3.mean() if not ROI_derived_1.empty else 0,\n",
    "    )\n",
    "\n",
    "# state_definition = [progress, measured_power, previous_PCAP, 'PAPI_L3_TCA', 'PAPI_TOT_INS', 'PAPI_TOT_CYC', 'PAPI_RES_STL', 'PAPI_L3_TCM']\n",
    "initial_progress = 0\n",
    "initial_power = 40\n",
    "initial_PAPI = np.zeros(5)\n",
    "state = (initial_progress,initial_power,initial_PAPI)\n",
    "t1 = float('-inf')\n",
    "for app in training_data.keys():\n",
    "    for trace in training_data[app].keys():\n",
    "        pcap_data = training_data[app][trace]['PCAP']\n",
    "        for i, row in pcap_data.iterrows():\n",
    "            t2 = row['time']\n",
    "            \n",
    "            # Get current state\n",
    "            state = get_state(training_data, app, trace, t1, t2)\n",
    "            \n",
    "            # Get next state (look ahead to next row)\n",
    "            if i + 1 < len(pcap_data):\n",
    "                t3 = pcap_data.iloc[i + 1]['time']\n",
    "                next_state = get_state(training_data, app, trace, t2, t3)\n",
    "            else:\n",
    "                next_state = state  # Use current state if it's the last row\n",
    "            \n",
    "            action = row['value']  # Assuming PCAP is in the 'value' column\n",
    "            \n",
    "            # Calculate the reward\n",
    "            reward = env.reward(state[0], action, next_state[0], state[1])\n",
    "            \n",
    "            # Add to training dataset\n",
    "            training_dataset.append((state, action, reward, next_state))\n",
    "            \n",
    "            t1 = t2\n",
    "\n",
    "\n",
    "# Define the CSV file name and path\n",
    "csv_file_name = 'training_dataset.csv'\n",
    "csv_file_path = os.path.join(DATA_DIR, csv_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "T_S = 1\n",
    "ACTIONS = [78.0, 83.0, 89.0, 95.0, 101.0, 107.0, 112.0, 118.0, 124.0, 130.0, 136.0, 141.0, 147.0, 153.0, 159.0, 165.0]\n",
    "exec_steps = 10000    \n",
    "TOTAL_ACTIONS = len(ACTIONS)                                                                                                  # Total clock cycles needed for the execution of program.\n",
    "ACTION_MIN = min(ACTIONS)                                                                                                    # Minima of control space\n",
    "ACTION_MAX = max(ACTIONS)                                                                                                     # Maxima of control space\n",
    "ACT_MID = ACTION_MIN + (ACTION_MAX - ACTION_MIN) / 2                                                                    # Midpoint of the control space to compute the normalized action space                           \n",
    "OBS_MIN = np.zeros((5,))  # Shape should be (7,)\n",
    "OBS_MAX = np.array([300,165,1,1,1])                                                                                 # Minima of observation space\n",
    "OBS_MID = OBS_MIN + (OBS_MAX - OBS_MIN) / 2\n",
    "EXEC_ITERATIONS = 10000\n",
    "TOTAL_OBS = OBS_MAX - OBS_MIN\n",
    "OBS_ONEHOT = 'onehot'\n",
    "OBS_RANDOM = 'random'\n",
    "OBS_SMOOTH = 'smooth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SYS(object):\n",
    "    def __init__(self,observation_type=OBS_ONEHOT,dim_obs=5,teps=0.0):\n",
    "        super(SYS,self).__init__()\n",
    "        self.num_actions = TOTAL_ACTIONS\n",
    "        self.action_space = gym.spaces.Discrete(len(ACTIONS)) \n",
    "        self.actions = ACTIONS  \n",
    "        self.observation_space = gym.spaces.Box(low=OBS_MIN, high=OBS_MAX, shape=(5,), dtype=np.float32)  # Infinite observation space with 8 dimensions\n",
    "        self.dim_obs = dim_obs\n",
    "    \n",
    "    def reward(self, s, a, ns, measured_power):\n",
    "        \"\"\" \n",
    "        Returns the reward (float)\n",
    "        \"\"\"\n",
    "        if ns > 0:\n",
    "            reward = ns/(a**2+1)\n",
    "        else:\n",
    "            reward = -100\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighting_only = False\n",
    "dataset_composition = 'random'\n",
    "dataset_size = 1000\n",
    "env_type = 'random'\n",
    "env = SYS(observation_type=env_type, dim_obs=5, teps=0)\n",
    "training_dataset = []\n",
    "old_PCAP = 0\n",
    "action_set = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_observations(env):\n",
    "    obs = []\n",
    "    for s in range(env.num_states):\n",
    "        obs.append(env.observation(s))\n",
    "    return np.stack(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNetwork(torch.nn.Module):\n",
    "  def __init__(self, env, layers=[20,20]):\n",
    "    super(FCNetwork, self).__init__()\n",
    "    # self.all_observations = torch.tensor(stack_observations(env), dtype=torch.float32)\n",
    "    dim_input = env.dim_obs\n",
    "    dim_output = env.num_actions\n",
    "    net_layers = []\n",
    "\n",
    "    dim = dim_input\n",
    "    for i, layer_size in enumerate(layers):\n",
    "      net_layers.append(torch.nn.Linear(dim, layer_size))\n",
    "      net_layers.append(torch.nn.ReLU())\n",
    "      dim = layer_size\n",
    "    net_layers.append(torch.nn.Linear(dim, dim_output))\n",
    "    self.layers = net_layers\n",
    "    self.network = torch.nn.Sequential(*net_layers)\n",
    "\n",
    "  # def forward(self, states):\n",
    "  #   observations = torch.index_select(self.all_observations, 0, states)\n",
    "  #   return self.network(observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensors(list_of_tensors, list_of_indices):\n",
    "  s, a, ns, r = [], [], [], []\n",
    "  for idx in list_of_indices:\n",
    "    s.append(list_of_tensors[idx][0])\n",
    "    a.append(list_of_tensors[idx][1])\n",
    "    r.append(list_of_tensors[idx][2])\n",
    "    ns.append(list_of_tensors[idx][3])\n",
    "  s = np.array(s)\n",
    "  a = np.array(a)\n",
    "  ns = np.array(ns)\n",
    "  r = np.array(r)\n",
    "  return s, a, ns, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_qvalues_cql_sampled(env, s, a, target_values, network, optimizer, cql_alpha=0.1, num_steps=50, weights=None):\n",
    "    # train with a sampled dataset\n",
    "    target_qvalues = torch.tensor(target_values, dtype=torch.float32)\n",
    "    s = torch.tensor(s, dtype=torch.int64)\n",
    "    a = torch.tensor(a, dtype=torch.int64)\n",
    "    pred_qvalues = network(s)\n",
    "    logsumexp_qvalues = torch.logsumexp(pred_qvalues, dim=-1)\n",
    "\n",
    "    pred_qvalues = pred_qvalues.gather(1, a.reshape(-1,1)).squeeze()\n",
    "    cql_loss = logsumexp_qvalues - pred_qvalues\n",
    "\n",
    "    loss = torch.mean((pred_qvalues - target_qvalues)**2)\n",
    "    loss = loss + cql_alpha * torch.mean(cql_loss)\n",
    "\n",
    "    network.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    pred_qvalues = network(torch.arange(env.num_states))\n",
    "    return pred_qvalues.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_qvalues_cql(q_values, network, optimizer, num_steps=50, cql_alpha=0.1, weights=None):\n",
    "    # regress onto q_values (aka projection)\n",
    "    q_values_tensor = torch.tensor(q_values, dtype=torch.float32)\n",
    "    for _ in range(num_steps):\n",
    "       # Eval the network at each state\n",
    "      pred_qvalues = network(torch.arange(q_values.shape[0]))\n",
    "      if weights is None:\n",
    "        loss = torch.mean((pred_qvalues - q_values_tensor)**2)\n",
    "      else:\n",
    "        loss = torch.mean(weights*(pred_qvalues - q_values_tensor)**2)\n",
    "\n",
    "      # Add cql_loss\n",
    "      # You can have two variants of this loss, one where data q-values\n",
    "      # also maximized (CQL-v2), and one where only the large Q-values\n",
    "      # are pushed down (CQL-v1) as covered in the tutorial\n",
    "      cql_loss = torch.logsumexp(pred_qvalues, dim=-1, keepdim=True) # - pred_qvalues\n",
    "      loss = loss + cql_alpha * torch.mean(weights * cql_loss)\n",
    "      network.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "    return pred_qvalues.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_backup_sparse_sampled(env, q_values, s, a, ns, r, discount=0.99):\n",
    "  q_values_ns = q_values[ns, :]\n",
    "  values = np.max(q_values_ns, axis=-1)\n",
    "  target_value = r + discount * values\n",
    "  return target_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conservative_q_iteration(env,\n",
    "                             network,\n",
    "                             num_itrs=100,\n",
    "                             project_steps=50,\n",
    "                             cql_alpha=0.1,\n",
    "                             render=False,\n",
    "                             weights=None,\n",
    "                             sampled=False,\n",
    "                             training_dataset=None,\n",
    "                             **kwargs):\n",
    "  \"\"\"\n",
    "  Runs Conservative Q-iteration.\n",
    "\n",
    "  Args:\n",
    "    env: A GridEnv object.\n",
    "    num_itrs (int): Number of FQI iterations to run.\n",
    "    project_steps (int): Number of gradient steps used for projection.\n",
    "    cql_alpha (float): Value of weight on the CQL coefficient.\n",
    "    render (bool): If True, will plot q-values after each iteration.\n",
    "    sampled (bool): Whether to use sampled datasets for training or not.\n",
    "    training_dataset (list): list of (s, a, r, ns) pairs\n",
    "  \"\"\"\n",
    "\n",
    "  optimizer = torch.optim.Adam(network.parameters(), lr=1e-3)\n",
    "  weights_tensor = None\n",
    "  if weights is not None:\n",
    "    weights_tensor = torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "  q_values = np.zeros((dS, dA)) #Initializing the Q-values for getting the target values\n",
    "  for i in range(num_itrs):\n",
    "    for j in range(project_steps):\n",
    "      training_idx = np.random.choice(np.arange(len(training_dataset)), size=128)\n",
    "      s, a, ns, r = get_tensors(training_dataset, training_idx)\n",
    "      target_values = q_backup_sparse_sampled(env, q_values, s, a, ns, r, **kwargs)\n",
    "      intermed_values = project_qvalues_cql_sampled(\n",
    "          env, s, a, target_values, network, optimizer,\n",
    "          cql_alpha=cql_alpha, weights=None,\n",
    "      )\n",
    "      if j == project_steps - 1:\n",
    "        q_values = intermed_values\n",
    "  return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m (weighting_only)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Run Q-iteration\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m q_values \u001b[38;5;241m=\u001b[39m \u001b[43mconservative_q_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mnum_itrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcql_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcql_alpha_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43msampled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mweighting_only\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mtraining_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Compute and plot the value function\u001b[39;00m\n\u001b[1;32m     18\u001b[0m v_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(q_values, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[20], line 29\u001b[0m, in \u001b[0;36mconservative_q_iteration\u001b[0;34m(env, network, num_itrs, project_steps, cql_alpha, render, weights, sampled, training_dataset, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m   weights_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(weights, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 29\u001b[0m q_values \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mzeros((dS, dA)) \u001b[38;5;66;03m#Initializing the Q-values for getting the target values\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_itrs):\n\u001b[1;32m     31\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(project_steps):\n",
      "Cell \u001b[0;32mIn[20], line 29\u001b[0m, in \u001b[0;36mconservative_q_iteration\u001b[0;34m(env, network, num_itrs, project_steps, cql_alpha, render, weights, sampled, training_dataset, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m   weights_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(weights, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 29\u001b[0m q_values \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mzeros((dS, dA)) \u001b[38;5;66;03m#Initializing the Q-values for getting the target values\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_itrs):\n\u001b[1;32m     31\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(project_steps):\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/RL_course/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/RL_course/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#@title Run conservative Q-iteration (or CQL) with finite data\n",
    "\n",
    "# Use a tabular or feedforward NN approximator\n",
    "network = FCNetwork(env, layers=[20, 20])\n",
    "#network = TabularNetwork(env)\n",
    "\n",
    "cql_alpha_val = 0.1 # @param {type:\"slider\", min:0.0, max:10.0, step:0.01}\n",
    "weights = None\n",
    "print (weighting_only)\n",
    "# Run Q-iteration\n",
    "q_values = conservative_q_iteration(env, network,\n",
    "                                    num_itrs=100, discount=0.95, cql_alpha=cql_alpha_val,\n",
    "                                    weights=weights, render=False,\n",
    "                                    sampled=not(weighting_only),\n",
    "                                    training_dataset=training_data)\n",
    "\n",
    "# Compute and plot the value function\n",
    "v_values = np.max(q_values, axis=1)\n",
    "# plot_s_values(env, v_values, title='Values')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "offline_tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
