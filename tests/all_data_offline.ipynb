{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ruamel.yaml import YAML\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.patches import Rectangle, Polygon\n",
    "from IPython.display import clear_output\n",
    "import torch\n",
    "import sys\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import math\n",
    "import warnings\n",
    "import os\n",
    "import math\n",
    "import csv\n",
    "import utils\n",
    "import DDPG\n",
    "import gym\n",
    "import BCQ\n",
    "import argparse\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_power_with_wraparound(current, previous, time_diff, wraparound_value=262143.328850):\n",
    "    diff = current - previous\n",
    "    if diff < 0:  # Wraparound detected\n",
    "        diff = (wraparound_value - previous) + current\n",
    "    return diff / time_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_power(pubEnergy):\n",
    "    power = {}\n",
    "    geopm_sensor0 = geopm_sensor1 = pd.DataFrame({'timestamp':[],'value':[]})\n",
    "    for i,row in pubEnergy.iterrows():\n",
    "        if i%2 == 0:\n",
    "            geopm_sensor0 = pd.concat([geopm_sensor0, pd.DataFrame([{'timestamp': row['time'], 'value': row['value']}])], ignore_index=True)\n",
    "        else:\n",
    "            geopm_sensor1 = pd.concat([geopm_sensor1, pd.DataFrame([{'timestamp': row['time'], 'value': row['value']}])], ignore_index=True)\n",
    "\n",
    "\n",
    "    power['geopm_power_0'] = pd.DataFrame({\n",
    "        'timestamp': geopm_sensor0['timestamp'][1:],  # Add timestamps\n",
    "        'power': [\n",
    "            calculate_power_with_wraparound(\n",
    "                geopm_sensor0['value'][i],\n",
    "                geopm_sensor0['value'][i-1],\n",
    "                geopm_sensor0['timestamp'][i] - geopm_sensor0['timestamp'][i-1]\n",
    "            ) for i in range(1, len(geopm_sensor0))\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    # Apply the same logic to geopm_power_1\n",
    "    power['geopm_power_1'] = pd.DataFrame({\n",
    "        'timestamp': geopm_sensor1['timestamp'][1:],  # Add timestamps\n",
    "        'power': [\n",
    "            calculate_power_with_wraparound(\n",
    "                geopm_sensor1['value'][i],\n",
    "                geopm_sensor1['value'][i-1],\n",
    "                geopm_sensor1['timestamp'][i] - geopm_sensor1['timestamp'][i-1]\n",
    "            ) for i in range(1, len(geopm_sensor1))\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    min_length = min(len(power['geopm_power_0']), len(power['geopm_power_1']))\n",
    "    geopm_power_0 = power['geopm_power_0'][:min_length]\n",
    "    geopm_power_1 = power['geopm_power_1'][:min_length]\n",
    "\n",
    "    average_power = pd.DataFrame({\n",
    "        'timestamp': geopm_power_0['timestamp'],  # Use the timestamp from geopm_power_0\n",
    "        'average_power': [(p0 + p1) / 2 for p0, p1 in zip(geopm_power_0['power'], geopm_power_1['power'])]\n",
    "    })\n",
    "    average_power['elapsed_time'] = average_power['timestamp'] - average_power['timestamp'].iloc[0]\n",
    "    power['average_power'] = average_power\n",
    "    return power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_progress(progress_data, energy_data):\n",
    "    Progress_DATA = {} \n",
    "    progress_sensor = pd.DataFrame(progress_data)\n",
    "    first_sensor_point = min(energy_data['average_power']['timestamp'].iloc[0], progress_sensor['time'][0])\n",
    "    progress_sensor['elapsed_time'] = progress_sensor['time'] - first_sensor_point  # New column for elapsed time\n",
    "    # progress_sensor = progress_sensor.set_index('elapsed_time')\n",
    "    performance_elapsed_time = progress_sensor.elapsed_time\n",
    "    # Add performance_frequency as a new column in progress_sensor\n",
    "    frequency_values = [\n",
    "        progress_data['value'].iloc[t] / (performance_elapsed_time[t] - performance_elapsed_time[t-1]) for t in range(1, len(performance_elapsed_time))\n",
    "    ]\n",
    "    \n",
    "    # Ensure the frequency_values length matches the index length\n",
    "    frequency_values = [0] + frequency_values  # Prepend a 0 for the first index\n",
    "    progress_sensor['frequency'] = frequency_values\n",
    "    upsampled_timestamps= energy_data['average_power']['timestamp']\n",
    "    \n",
    "    # true_count = (progress_sensor['time'] <= upsampled_timestamps.iloc[0]).sum()\n",
    "\n",
    "    progress_frequency_median = pd.DataFrame({'median': np.nanmedian(progress_sensor['frequency'].where(progress_sensor['time'] <= upsampled_timestamps.iloc[0])), 'timestamp': upsampled_timestamps.iloc[0]}, index=[0])\n",
    "    for t in range(1, len(upsampled_timestamps)):\n",
    "        progress_frequency_median = pd.concat([progress_frequency_median, pd.DataFrame({'median': [np.nanmedian(progress_sensor['frequency'].where((progress_sensor['time'] >= upsampled_timestamps.iloc[t-1]) & (progress_sensor['time'] <= upsampled_timestamps.iloc[t])))],\n",
    "        'timestamp': [upsampled_timestamps.iloc[t]]})], ignore_index=True)\n",
    "    progress_frequency_median['elapsed_time'] = progress_frequency_median['timestamp'] - progress_frequency_median['timestamp'].iloc[0]\n",
    "    # Assign progress_frequency_median as a new column\n",
    "    Progress_DATA['progress_sensor'] = progress_sensor\n",
    "    Progress_DATA['progress_frequency_median'] = progress_frequency_median\n",
    "    return Progress_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_papi(PAPI_data):\n",
    "    PAPI = {}\n",
    "    for scope in PAPI_data['scope'].unique():\n",
    "        # Extract the string between the 3rd and 4th dots\n",
    "        scope_parts = scope.split('.')\n",
    "        if len(scope_parts) > 4:  # Ensure there are enough parts\n",
    "            extracted_scope = scope_parts[3]\n",
    "            # Aggregate the data for the extracted scope using pd.concat\n",
    "            PAPI[extracted_scope] = PAPI_data[PAPI_data['scope'] == scope]\n",
    "            instantaneous_values = [0] + [PAPI[extracted_scope]['value'].iloc[k] - PAPI[extracted_scope]['value'].iloc[k-1] for k in range(1,len(PAPI[extracted_scope]))]\n",
    "            # Normalize the instantaneous values between 0 and 10\n",
    "            # min_val = min(instantaneous_values)\n",
    "            # max_val = max(instantaneous_values)\n",
    "            PAPI[extracted_scope]['instantaneous_value'] = instantaneous_values\n",
    "            PAPI[extracted_scope]['elapsed_time'] = PAPI[extracted_scope]['time'] - PAPI[extracted_scope]['time'].iloc[0]\n",
    "    return PAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(traces):\n",
    "    normalized_PAPI = {}\n",
    "    max_value = {'PAPI_L3_TCA': float('-inf'), 'PAPI_TOT_INS': float('-inf'), 'PAPI_TOT_CYC': float('-inf'), 'PAPI_RES_STL': float('-inf'), 'PAPI_L3_TCM': float('-inf')}\n",
    "    min_value = {'PAPI_L3_TCA': float('inf'), 'PAPI_TOT_INS': float('inf'), 'PAPI_TOT_CYC': float('inf'), 'PAPI_RES_STL': float('inf'), 'PAPI_L3_TCM': float('inf')}\n",
    "    for app in traces.keys():\n",
    "        for trace in traces[app].keys():\n",
    "            for scope in traces[app][trace]['papi'].keys():\n",
    "                max_value[scope] = max(max_value[scope],max(traces[app][trace]['papi'][scope]['instantaneous_value']))\n",
    "                min_value[scope] = min(min_value[scope],min(traces[app][trace]['papi'][scope]['instantaneous_value']))\n",
    "    for app in traces.keys():\n",
    "        for trace in traces[app].keys():\n",
    "            for scope in traces[app][trace]['papi'].keys():\n",
    "                traces[app][trace]['papi'][scope]['normalized_value'] = [(value - min_value[scope]) / (max_value[scope] - min_value[scope]) * 10 for value in traces[app][trace]['papi'][scope]['instantaneous_value']]\n",
    "    return traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_PCAP(PCAP_data):\n",
    "    for row in PCAP_data.iterrows():\n",
    "        if row[1]['time'] == 0:\n",
    "            PCAP_data = PCAP_data.drop(row[0])\n",
    "\n",
    "\n",
    "    PCAP_data['elapsed_time'] = PCAP_data['time'] - PCAP_data['time'].iloc[0]\n",
    "    return PCAP_data\n",
    "\n",
    "# def get_data_dir():\n",
    "#     current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "#     return os.path.join(current_dir, \"experiment_data\", \"data_generation\")\n",
    "\n",
    "DATA_DIR = \"/home/cc/summer2024/tests/experiment_data/data_generation\"\n",
    "root,folders,files = next(os.walk(DATA_DIR))\n",
    "training_data = {}\n",
    "for APP in folders:\n",
    "    # fig, axs = plt.subplots(2,1,figsize=(12,10))\n",
    "    # fig_PAPI, axs_PAPI = plt.subplots(5,1,figsize=(12,16))\n",
    "    # print(APP)\n",
    "    APP_DIR = os.path.join(DATA_DIR, APP)\n",
    "    # print(APP_DIR)\n",
    "    training_data[APP] = {}\n",
    "    # training_data[APP]['data'] = {} \n",
    "    for file in next(os.walk(APP_DIR))[2]:\n",
    "        # print(file)\n",
    "        training_data[APP][file] = {}\n",
    "        if file.endswith('.tar'):\n",
    "            tar_path = os.path.join(APP_DIR, file)\n",
    "            extract_dir = os.path.join(APP_DIR, file[:-4])  \n",
    "            \n",
    "            if not os.path.exists(extract_dir):\n",
    "                os.makedirs(extract_dir)\n",
    "            \n",
    "            # Extract the tar file\n",
    "            with tarfile.open(tar_path, 'r') as tar:\n",
    "                tar.extractall(path=extract_dir)\n",
    "            \n",
    "            # print(f\"Extracted {file} to {extract_dir}\")\n",
    "        pubProgress = pd.read_csv(f'{extract_dir}/progress.csv')\n",
    "        pubEnergy = pd.read_csv(f'{extract_dir}/energy.csv')\n",
    "        pubPAPI = pd.read_csv(f'{extract_dir}/papi.csv')\n",
    "        pubPCAP = pd.read_csv(f'{extract_dir}/PCAP_file.csv')\n",
    "        # with open(f'{extract_dir}/parameters.yaml', 'r') as f:\n",
    "        #     yaml = YAML(typ='safe', pure=True)\n",
    "        #     parameters = yaml.load(f)\n",
    "        #     PCAP = parameters['PCAP']\n",
    "        # training_data['data']['PCAP'] = pd.read_csv(f'{extract_dir}/PCAP_file.csv')\n",
    "        training_data[APP][file]['power'] = compute_power(pubEnergy)\n",
    "        training_data[APP][file]['progress'] = measure_progress(pubProgress,training_data[APP][file]['power'])\n",
    "        training_data[APP][file]['papi'] = collect_papi(pubPAPI)\n",
    "        training_data[APP][file]['PCAP'] = generate_PCAP(pubPCAP)   \n",
    "        # print(training_data[APP][file]['PCAP'] )    \n",
    "training_data = normalize(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_S = 1\n",
    "ACTIONS = [78.0, 83.0, 89.0, 95.0, 101.0, 107.0, 112.0, 118.0, 124.0, 130.0, 136.0, 141.0, 147.0, 153.0, 159.0, 165.0]\n",
    "exec_steps = 10000    \n",
    "TOTAL_ACTIONS = len(ACTIONS)                                                                                                  # Total clock cycles needed for the execution of program.\n",
    "ACTION_MIN = min(ACTIONS)                                                                                                    # Minima of control space\n",
    "ACTION_MAX = max(ACTIONS)                                                                                                     # Maxima of control space\n",
    "ACT_MID = ACTION_MIN + (ACTION_MAX - ACTION_MIN) / 2                                                                    # Midpoint of the control space to compute the normalized action space\n",
    "# OBS_MAX = 300                                                                                                           # Maxima of observation space (performance)\n",
    "# OBS_MIN = 0                           \n",
    "OBS_MIN = np.zeros((7,))  # Shape should be (7,)\n",
    "OBS_MAX = np.array([300,165,10,10,10,10,10])                                                                                 # Minima of observation space\n",
    "OBS_MID = OBS_MIN + (OBS_MAX - OBS_MIN) / 2\n",
    "EXEC_ITERATIONS = 10000\n",
    "TOTAL_OBS = OBS_MAX - OBS_MIN\n",
    "# print(TOTAL_ACTIONS)\n",
    "OBS_ONEHOT = 'onehot'\n",
    "OBS_RANDOM = 'random'\n",
    "OBS_SMOOTH = 'smooth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighting_only = False\n",
    "dataset_composition = 'random'\n",
    "dataset_size = 1000\n",
    "env_type = 'random'\n",
    "\n",
    "\n",
    "# training_data_csv = pd.read_csv('./merged_data.csv')\n",
    "# PCAP = 0\n",
    "# CURRENT_PRO = 0\n",
    "# NEXT_PRO = 0\n",
    "training_dataset = []\n",
    "old_PCAP = 0\n",
    "action_set = []\n",
    "# Helper function to get state\n",
    "def get_roi_data(df, time_column, start_time, end_time):\n",
    "    return df[(df[time_column] > start_time) & (df[time_column] <= end_time)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SYS(object):\n",
    "    def __init__(self,observation_type=OBS_ONEHOT,dim_obs=1,teps=0.0):\n",
    "        super(SYS,self).__init__()\n",
    "\n",
    "        self.num_actions = TOTAL_ACTIONS\n",
    "        self.action_space = gym.spaces.Discrete(len(ACTIONS))  # Use the length of the ACTIONS list for discrete actions\n",
    "        # Map the selected index to the corresponding action\n",
    "        self.actions = ACTIONS  # Store the actions for later use\n",
    "        self.observation_space = gym.spaces.Box(low=OBS_MIN, high=OBS_MAX, shape=(7,), dtype=np.float32)  # Infinite observation space with 8 dimensions\n",
    "\n",
    "    \n",
    "    def reward(self, s, a, ns, measured_power):\n",
    "        \"\"\" \n",
    "        Returns the reward (float)\n",
    "        \"\"\"\n",
    "        # measured_power = A[cluster] * a + B[cluster]\n",
    "        if ns > 0:\n",
    "            # self.current_step += ns\n",
    "            # reward = - 5*a\n",
    "            # reward = 2*ns/(((a)/measured_power)+measured_power) # Check the behaviour across the states\n",
    "            reward = -ns/(2*a**2+1)\n",
    "            # reward = 5*a\n",
    "        else:\n",
    "            reward = -100\n",
    "        # print(reward)\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SYS(observation_type=env_type, dim_obs=8, teps=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset has been saved to /home/cc/summer2024/tests/experiment_data/data_generation/training_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "def get_state(training_data, app, trace, start_time, end_time):\n",
    "    ROI_progress = get_roi_data(training_data[app][trace]['progress']['progress_frequency_median'], 'timestamp', start_time, end_time)\n",
    "    ROI_measured_power = get_roi_data(training_data[app][trace]['power']['average_power'], 'timestamp', start_time, end_time)\n",
    "    ROI_L3_TCA = get_roi_data(training_data[app][trace]['papi']['PAPI_L3_TCA'], 'time', start_time, end_time)\n",
    "    ROI_TOT_INS = get_roi_data(training_data[app][trace]['papi']['PAPI_TOT_INS'], 'time', start_time, end_time)\n",
    "    ROI_TOT_CYC = get_roi_data(training_data[app][trace]['papi']['PAPI_TOT_CYC'], 'time', start_time, end_time)\n",
    "    ROI_RES_STL = get_roi_data(training_data[app][trace]['papi']['PAPI_RES_STL'], 'time', start_time, end_time)\n",
    "    ROI_L3_TCM = get_roi_data(training_data[app][trace]['papi']['PAPI_L3_TCM'], 'time', start_time, end_time)\n",
    "    \n",
    "    return (\n",
    "        ROI_progress['median'].mean() if not ROI_progress.empty else 0,\n",
    "        ROI_measured_power['average_power'].mean() if not ROI_measured_power.empty else 0,\n",
    "        ROI_L3_TCA['normalized_value'].mean() if not ROI_L3_TCA.empty else 0,\n",
    "        ROI_TOT_INS['normalized_value'].mean() if not ROI_TOT_INS.empty else 0,\n",
    "        ROI_TOT_CYC['normalized_value'].mean() if not ROI_TOT_CYC.empty else 0,\n",
    "        ROI_RES_STL['normalized_value'].mean() if not ROI_RES_STL.empty else 0,\n",
    "        ROI_L3_TCM['normalized_value'].mean() if not ROI_L3_TCM.empty else 0\n",
    "    )\n",
    "\n",
    "# state_definition = [progress, measured_power, previous_PCAP, 'PAPI_L3_TCA', 'PAPI_TOT_INS', 'PAPI_TOT_CYC', 'PAPI_RES_STL', 'PAPI_L3_TCM']\n",
    "initial_progress = 0\n",
    "initial_power = 40\n",
    "initial_PAPI = np.zeros(5)\n",
    "state = (initial_progress,initial_power,initial_PAPI)\n",
    "t1 = float('-inf')\n",
    "for app in training_data.keys():\n",
    "    for trace in training_data[app].keys():\n",
    "        pcap_data = training_data[app][trace]['PCAP']\n",
    "        for i, row in pcap_data.iterrows():\n",
    "            t2 = row['time']\n",
    "            \n",
    "            # Get current state\n",
    "            state = get_state(training_data, app, trace, t1, t2)\n",
    "            \n",
    "            # Get next state (look ahead to next row)\n",
    "            if i + 1 < len(pcap_data):\n",
    "                t3 = pcap_data.iloc[i + 1]['time']\n",
    "                next_state = get_state(training_data, app, trace, t2, t3)\n",
    "            else:\n",
    "                next_state = state  # Use current state if it's the last row\n",
    "            \n",
    "            action = row['value']  # Assuming PCAP is in the 'value' column\n",
    "            \n",
    "            # Calculate the reward\n",
    "            reward = env.reward(state[0], action, next_state[0], state[1])\n",
    "            \n",
    "            # Add to training dataset\n",
    "            training_dataset.append((state, action, reward, next_state))\n",
    "            \n",
    "            t1 = t2\n",
    "\n",
    "\n",
    "# Define the CSV file name and path\n",
    "csv_file_name = 'training_dataset.csv'\n",
    "csv_file_path = os.path.join(DATA_DIR, csv_file_name)\n",
    "\n",
    "# Open the CSV file in write mode\n",
    "with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "    # Create a CSV writer object\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    \n",
    "    # Write the header\n",
    "    csv_writer.writerow(['Progress', 'Power', 'L3_TCA', 'TOT_INS', 'TOT_CYC', 'RES_STL', 'L3_TCM', \n",
    "                         'Action', 'Reward', \n",
    "                         'Next_Progress', 'Next_Power', 'Next_L3_TCA', 'Next_TOT_INS', 'Next_TOT_CYC', 'Next_RES_STL', 'Next_L3_TCM'])\n",
    "    \n",
    "    # Write the data\n",
    "    for state, action, reward, next_state in training_dataset:\n",
    "        row = list(state) + [action, reward] + list(next_state)\n",
    "        csv_writer.writerow(row)\n",
    "\n",
    "print(f\"Training dataset has been saved to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = \"/home/cc/summer2024/tests/experiment_data/data_generation/training_dataset_filtered.csv\"\n",
    "training_dataset = []\n",
    "training_dataset = pd.read_csv(csv_file_path).values.tolist()  # Convert DataFrame to list of tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_policy(policy, env_name, seed, eval_episodes=10):\n",
    "\teval_env = gym.make(env_name)\n",
    "\teval_env.seed(seed + 100)\n",
    "\n",
    "\tavg_reward = 0.\n",
    "\tfor _ in range(eval_episodes):\n",
    "\t\tstate, done = eval_env.reset(), False\n",
    "\t\twhile not done:\n",
    "\t\t\taction = policy.select_action(np.array(state))\n",
    "\t\t\tstate, reward, done, _ = eval_env.step(action)\n",
    "\t\t\tavg_reward += reward\n",
    "\n",
    "\tavg_reward /= eval_episodes\n",
    "\n",
    "\tprint(\"---------------------------------------\")\n",
    "\tprint(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n",
    "\tprint(\"---------------------------------------\")\n",
    "\treturn avg_reward\n",
    "\n",
    "\n",
    "# Trains BCQ offline\n",
    "def train_BCQ(state_dim, action_dim, max_action, device, args, replay_buffer):\n",
    "\t# For saving files\n",
    "\tsetting = f\"{args.env}_{args.seed}\"\n",
    "\t# buffer_name = f\"{args.buffer_name}_{setting}\"\n",
    "\n",
    "\t# Initialize policy\n",
    "    \n",
    "\tpolicy = BCQ.BCQ(state_dim, action_dim, max_action, device, args.discount, args.tau, args.lmbda, args.phi, seed=args.seed)\n",
    "\n",
    "\t# Load buffer\n",
    "\t# replay_buffer = utils.ReplayBuffer(state_dim, action_dim, device)\n",
    "\t# replay_buffer.load(f\"./buffers/{buffer_name}\")\n",
    "\t\n",
    "\t# evaluations = []\n",
    "\t# episode_num = 0\n",
    "\t# done = True \n",
    "\ttraining_iters = 0\n",
    "\n",
    "\twhile training_iters < args.max_timesteps: \n",
    "\t\tpol_vals = policy.train(replay_buffer, iterations=int(args.eval_freq), batch_size=args.batch_size)\n",
    "\t\tif \"results\" not in os.listdir():  # Check for the directory without \"./\"\n",
    "\t\t\tos.makedirs(\"results\", exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\t\ttorch.save(policy, f\"./results/BCQ_{setting}.pt\")  # Save the results\n",
    "\n",
    "\t\ttraining_iters += args.eval_freq\n",
    "\t\tprint(f\"Training iterations: {training_iters}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = \"SYS\"               # OpenAI gym environment name\n",
    "seed = 0                   # Sets Gym, PyTorch and Numpy seeds\n",
    "buffer_name = \"Robust\"    # Prepends name to filename\n",
    "eval_freq = 5e3           # How often (time steps) we evaluate\n",
    "max_timesteps = 1e4       # Max time steps to run environment or train for (this defines buffer size)\n",
    "start_timesteps = 25e3    # Time steps initial random policy is used before training behavioral\n",
    "rand_action_p = 0.3       # Probability of selecting random action during batch generation\n",
    "gaussian_std = 0.3        # Std of Gaussian exploration noise (Set to 0.1 if DDPG trains poorly)\n",
    "batch_size = 100          # Mini batch size for networks\n",
    "discount = 0.99           # Discount factor\n",
    "tau = 0.005               # Target network update rate\n",
    "lmbda = 0.75              # Weighting for clipped double Q-learning in BCQ\n",
    "phi = 0.05                # Max perturbation hyper-parameter for BCQ\n",
    "train_behavioral = False   # If true, train behavioral (DDPG)\n",
    "generate_buffer = False    # If true, generate buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total T:98 Episode Num: 1 Episode T: 0 Reward: -0.791\n",
      "Total T:169 Episode Num: 2 Episode T: 0 Reward: -0.766\n",
      "Total T:311 Episode Num: 3 Episode T: 0 Reward: -0.820\n",
      "Total T:654 Episode Num: 4 Episode T: 0 Reward: -0.776\n",
      "Total T:997 Episode Num: 5 Episode T: 0 Reward: -0.776\n",
      "Total T:1340 Episode Num: 6 Episode T: 0 Reward: -0.776\n",
      "Total T:1400 Episode Num: 7 Episode T: 0 Reward: -100.562\n",
      "Total T:1460 Episode Num: 8 Episode T: 0 Reward: -100.562\n",
      "Total T:1520 Episode Num: 9 Episode T: 0 Reward: -100.562\n",
      "Total T:1580 Episode Num: 10 Episode T: 0 Reward: -100.562\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m         episode_timesteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     41\u001b[0m         episode_num \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 43\u001b[0m train_BCQ(state_dim, action_dim, max_action, device, \u001b[43margs\u001b[49m, replay_buffer)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "BATCH_SIZE = 2500\n",
    "state_dim = 7\n",
    "action_dim = 1\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "max_action = ACTION_MAX\n",
    "max_timesteps = 2500\n",
    "episode_reward = 0\n",
    "episode_timesteps = 0\n",
    "episode_num = 0\n",
    "# policy = DDPG.DDPG(state_dim, action_dim, max_action, device)#, args.discount, args.tau)\n",
    "replay_buffer = utils.ReplayBuffer(state_dim, action_dim, device)\n",
    "# Assuming training_dataset is a list of tuples\n",
    "\n",
    "\t\n",
    "\n",
    "\n",
    "for i in range(len(training_dataset) - 1):  # Avoid index out of range\n",
    "    # current_state, action, reward, next_state = training_dataset[i]\n",
    "    current_state = training_dataset[i][:7]\n",
    "    action = training_dataset[i][7]\n",
    "    reward = training_dataset[i][8]\n",
    "    next_state = training_dataset[i][9:16]\n",
    "    next_state_info = training_dataset[i+1][:7]\n",
    "    # next_state_info,_,_,_, = training_dataset[i + 1]  # Access the next line\n",
    "    if next_state_info[0] == 0:\n",
    "        done = float(True)\n",
    "    else:\n",
    "        done = float(False)\n",
    "    replay_buffer.add(current_state, action, next_state, reward, done)\n",
    "    episode_reward += reward\n",
    "    # print(done)\n",
    "    if done == 1.0: \n",
    "        # +1 to account for 0 indexing. +0 on ep_timesteps since it will increment +1 even if done=True\n",
    "        print(f\"Total T:{i} Episode Num: {episode_num+1} Episode T: {episode_timesteps} Reward: {episode_reward:.3f}\")\n",
    "        # Reset environment\n",
    "        # state, done = env.reset(), False\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        episode_timesteps = 0\n",
    "        episode_num += 1\n",
    "    \n",
    "train_BCQ(state_dim, action_dim, max_action, device, args, replay_buffer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ANL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
